{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import arxiv\n",
    "\n",
    "PATH_ALL_PAPER_TRAIN: str = \"../sota/dataset/train\"\n",
    "PATH_ALL_PAPER_VALIDATION: str = \"../sota/dataset/validation\"\n",
    "\n",
    "all_paper_id_iter = Path(PATH_ALL_PAPER_TRAIN).glob(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cnt = 1222\n",
    "process_cnt_bak = process_cnt\n",
    "'''Will replaced by len() method later.'''\n",
    "\n",
    "all_paper_dict: dict = {}\n",
    "for paper_src in all_paper_id_iter:\n",
    "  all_paper_dict[paper_src.name] = {\"name\": None, \"title\": None}\n",
    "  all_paper_dict[paper_src.name][\"name\"] = paper_src.name\n",
    "  process_cnt -= 1\n",
    "  if process_cnt == 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sources/2005'] does exist!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkAndCreateFolder(dirpath: str,\n",
    "                         filename: str=None, \n",
    "                         basepath: str=\"sources\",\n",
    "                         demo: bool=False\n",
    "                        ) -> bool:\n",
    "  '''\n",
    "  return `True` if the file exists.\n",
    "  '''\n",
    "  base_path = Path(basepath)\n",
    "  folder_path = Path(basepath).joinpath(dirpath)\n",
    "  file_path = folder_path.joinpath(filename)\n",
    "  if not base_path.exists():\n",
    "    base_path.mkdir()\n",
    "  if not folder_path.exists():\n",
    "    # print(f\"['{folder_path}'] doesn't exist. Creating one for you...\")\n",
    "    print(f\"['{folder_path}'] doesn't exist. \", end=\"\" if not demo else \"\\n\")\n",
    "    if not demo:\n",
    "      print(\"Creating one for you...\")\n",
    "      folder_path.mkdir()\n",
    "  else:\n",
    "    if demo:\n",
    "      print(f\"['{folder_path}'] does exist!\")\n",
    "  \n",
    "  return file_path.exists()\n",
    "\n",
    "_paper_src_name = \"2005.05005v2\"\n",
    "checkAndCreateFolder(dirpath=_paper_src_name.split(\".\")[0], filename=_paper_src_name, demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_src(arxiv_id: list):\n",
    "  '''\n",
    "  Check if the directory exists, using arXiv API to download the `src`(latex included).\n",
    "  \n",
    "  ## returns\n",
    "  paper_info: dict\n",
    "  '''\n",
    "  search_by_id = arxiv.Search(id_list=arxiv_id)\n",
    "  client = arxiv.Client(page_size=500, delay_seconds=3, num_retries=3)\n",
    "  pbar = tqdm(client.results(search_by_id), \n",
    "              # total=probe_interval.stop - probe_interval.start,\n",
    "              total=len(arxiv_id),\n",
    "              miniters=1,\n",
    "              mininterval=0,\n",
    "              desc=\"Starting process...\"\n",
    "              )\n",
    "  skipped_list = []\n",
    "  paper_info: dict = {}\n",
    "\n",
    "  for paper in pbar: \n",
    "    paper_id = paper.get_short_id()\n",
    "    pbar.set_description(f\"Processing {paper_id}\")\n",
    "    paper_info[paper_id] = {}\n",
    "    paper_info[paper_id][\"name\"] = paper_id\n",
    "    paper_info[paper_id][\"title\"] = paper.title\n",
    "    if not checkAndCreateFolder(dirpath=paper_id.split(\".\")[0], filename=paper_id):\n",
    "      # Skip the task if the file already exist.\n",
    "      pbar.write(f\"{pbar.n+1}: [{paper_id}] {paper.title}\")\n",
    "      paper.download_source(dirpath=\"./sources/\" + paper_id.split(\".\")[0], filename=paper_id)\n",
    "    else:\n",
    "      skipped_list.append(paper_id)\n",
    "\n",
    "  # print(f\"{len(arxiv_id)} documents to process ({len(skipped_list)} were skipped).\")\n",
    "  pbar.write(f\"{len(arxiv_id)} documents to process ({len(skipped_list)} were skipped).\")\n",
    "\n",
    "  return paper_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Crawler Interval: [0, 400] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996f95c338c24de287ff65360e683fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 documents to process (400 were skipped).\n",
      "----- Crawler Interval: [400, 800] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0904d4984d594dbc9133d4cbc574deeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 documents to process (400 were skipped).\n",
      "----- Crawler Interval: [800, 1200] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e9aa4721254d0e95ce1b055d47a910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 documents to process (400 were skipped).\n",
      "----- Crawler Interval: [1200, 1222] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520c5978f34a4420874b6343639a57df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18: [1706.03358v3] Sliced Wasserstein Kernel for Persistence Diagrams\n",
      "19: [1304.5870v2] Parameterized Complexity of the Anchored k-Core Problem for Directed Graphs\n",
      "20: [2101.10837v1] Ikshana: A Theory of Human Scene Understanding Mechanism\n",
      "21: [2302.11813v1] Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification\n",
      "22: [2009.05475v2] Adversarial score matching and improved sampling for image generation\n",
      "22 documents to process (17 were skipped).\n"
     ]
    }
   ],
   "source": [
    "RECORD_FMT: str = \"csv\" # \"json\"\n",
    "ITER_STEP: int = 400\n",
    "start_flag: int = 0\n",
    "\n",
    "while start_flag <= process_cnt_bak:\n",
    "  if start_flag + ITER_STEP >= process_cnt_bak:\n",
    "    probe_interval = slice(start_flag, process_cnt_bak)\n",
    "  else:\n",
    "    probe_interval = slice(start_flag, start_flag + ITER_STEP)\n",
    "  paper_id_list = list(all_paper_dict.keys())[probe_interval]\n",
    "  print(f\"----- Crawler Interval: [{probe_interval.start}, {probe_interval.stop}] -----\")\n",
    "  part_paper_dict = download_src(paper_id_list)\n",
    "  start_flag += ITER_STEP\n",
    "\n",
    "  info_table = pd.DataFrame(part_paper_dict).T\n",
    "  info_table.to_csv(f\"record_{probe_interval.start}-{probe_interval.stop}.{RECORD_FMT}\")\n",
    "\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
