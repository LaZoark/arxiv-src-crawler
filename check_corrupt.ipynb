{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import arxiv\n",
    "from os import system as system_command\n",
    "import tarfile\n",
    "\n",
    "PATH_ALL_PAPER_TRAIN: str = \"../sota/dataset/train\"\n",
    "PATH_ALL_PAPER_VALIDATION: str = \"../sota/dataset/validation\"\n",
    "\n",
    "PATH_TO_DOWNLOAD: str = r\"sources\"\n",
    "\n",
    "all_paper_id_iter = Path(PATH_ALL_PAPER_TRAIN).glob(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cnt = 3200\n",
    "process_cnt_bak = process_cnt\n",
    "'''Will replaced by len() method later.'''\n",
    "\n",
    "all_paper_dict: dict = {}\n",
    "for paper_src in all_paper_id_iter:\n",
    "  all_paper_dict[paper_src.name] = {} # {\"name\": None, \"title\": None}\n",
    "  # all_paper_dict[paper_src.name][\"name\"] = paper_src.name\n",
    "  process_cnt -= 1\n",
    "  if process_cnt == 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sources/2006'] does exist!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkAndCreateFolder(dirpath: str,\n",
    "                         filename: str=None, \n",
    "                         basepath: str=\"sources\",\n",
    "                         demo: bool=False\n",
    "                        ) -> bool:\n",
    "  '''\n",
    "  return `True` if the file exists.\n",
    "  '''\n",
    "  base_path = Path(basepath)\n",
    "  folder_path = Path(basepath).joinpath(dirpath)\n",
    "  file_path = folder_path.joinpath(filename)\n",
    "  if not base_path.exists():\n",
    "    base_path.mkdir()\n",
    "  if not folder_path.exists():\n",
    "    # print(f\"['{folder_path}'] doesn't exist. Creating one for you...\")\n",
    "    print(f\"['{folder_path}'] doesn't exist. \", end=\"\" if not demo else \"\\n\")\n",
    "    if not demo:\n",
    "      print(\"Creating one for you...\")\n",
    "      folder_path.mkdir()\n",
    "  else:\n",
    "    if demo:\n",
    "      print(f\"['{folder_path}'] does exist!\")\n",
    "  \n",
    "  if file_path.exists():\n",
    "    return_code = system_command(f\"gunzip -t '{file_path}' > /dev/null\")\n",
    "    # tarfile.open()\n",
    "    if return_code:\n",
    "      print(f\"Error {return_code}. Removing corrupted file ('{file_path}'). Please run the download process later.\")\n",
    "      system_command(f\"rm {file_path}\")\n",
    "\n",
    "  return file_path.exists()\n",
    "\n",
    "# _paper_src_name = \"2005.05005v2\"\n",
    "# _paper_src_name = \"2206.09112v4\"\n",
    "_paper_src_name = \"2006.10380v2\"\n",
    "checkAndCreateFolder(dirpath=_paper_src_name.split(\".\")[0], filename=_paper_src_name, demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_src(arxiv_id: list, \n",
    "                 basepath: str=\"sources\"\n",
    "                 ) -> dict:\n",
    "  '''\n",
    "  Check if the directory exists, using arXiv API to download the `src`(latex included).\n",
    "  \n",
    "  ## returns\n",
    "  paper_info: dict\n",
    "  '''\n",
    "  search_by_id = arxiv.Search(id_list=arxiv_id)\n",
    "  client = arxiv.Client(page_size=500, delay_seconds=3, num_retries=3)\n",
    "  pbar = tqdm(client.results(search_by_id), \n",
    "              # total=probe_interval.stop - probe_interval.start,\n",
    "              total=len(arxiv_id),\n",
    "              miniters=1,\n",
    "              mininterval=0,\n",
    "              desc=\"Starting process...\"\n",
    "              )\n",
    "  skipped_list = []\n",
    "  paper_info: dict = {}\n",
    "\n",
    "  for paper in pbar: \n",
    "    paper_id = paper.get_short_id()\n",
    "    pbar.set_description(f\"Processing {paper_id}\")\n",
    "    paper_info[paper_id] = {}\n",
    "    paper_info[paper_id][\"name\"] = paper_id\n",
    "    paper_info[paper_id][\"title\"] = paper.title\n",
    "    if not checkAndCreateFolder(dirpath=paper_id.split(\".\")[0], filename=paper_id, basepath=PATH_TO_DOWNLOAD):\n",
    "      # Skip the task if the file already exist.\n",
    "      pbar.write(f\"{pbar.n+1}: [{paper_id}] {paper.title}\")\n",
    "      paper.download_source(dirpath=Path(basepath).joinpath(paper_id.split(\".\")[0]), filename=paper_id)\n",
    "    else:\n",
    "      skipped_list.append(paper_id)\n",
    "\n",
    "  pbar.write(f\"({len(skipped_list)}/{len(arxiv_id)} were skipped)\")\n",
    "\n",
    "  return paper_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Crawler Interval: [0, 400] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9c21e4166947fea4ffc0b54bd3a7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400/400 were skipped)\n",
      "----- Crawler Interval: [400, 800] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1652a6fe8934549bdf4d9c00100dcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400/400 were skipped)\n",
      "----- Crawler Interval: [800, 1200] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125a1acde6e548bb8b0de811d0bcb30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400/400 were skipped)\n",
      "----- Crawler Interval: [1200, 1600] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6e4a20fd7541698073b4cd601a5762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400/400 were skipped)\n",
      "----- Crawler Interval: [1600, 2000] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d82bc0bf58a4ad88b32e3042fdc6274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RECORD_FMT: str = \"csv\" # \"json\"\n",
    "ITER_STEP: int = 400\n",
    "start_flag: int = 0\n",
    "\n",
    "while start_flag <= process_cnt_bak:\n",
    "  if start_flag + ITER_STEP >= process_cnt_bak:\n",
    "    probe_interval = slice(start_flag, process_cnt_bak)\n",
    "  else:\n",
    "    probe_interval = slice(start_flag, start_flag + ITER_STEP)\n",
    "  paper_id_list = list(all_paper_dict.keys())[probe_interval]\n",
    "  print(f\"----- Crawler Interval: [{probe_interval.start}, {probe_interval.stop}] -----\")\n",
    "  part_paper_dict = download_src(arxiv_id=paper_id_list, basepath=PATH_TO_DOWNLOAD)\n",
    "  start_flag += ITER_STEP\n",
    "\n",
    "  # info_table = pd.DataFrame(part_paper_dict).T\n",
    "  # info_table.to_csv(f\"record_{probe_interval.start}-{probe_interval.stop}.{RECORD_FMT}\")\n",
    "\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
