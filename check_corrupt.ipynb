{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import arxiv\n",
    "\n",
    "PATH_ALL_PAPER_TRAIN: str = \"../sota/dataset/train\"\n",
    "PATH_ALL_PAPER_VALIDATION: str = \"../sota/dataset/validation\"\n",
    "\n",
    "PATH_TO_DOWNLOAD: str = r\"sources\"\n",
    "\n",
    "all_paper_id_iter = Path(PATH_ALL_PAPER_TRAIN).glob(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_cnt = 2100\n",
    "process_cnt_bak = process_cnt\n",
    "'''Will replaced by len() method later.'''\n",
    "\n",
    "all_paper_dict: dict = {}\n",
    "for paper_src in all_paper_id_iter:\n",
    "  all_paper_dict[paper_src.name] = {} # {\"name\": None, \"title\": None}\n",
    "  # all_paper_dict[paper_src.name][\"name\"] = paper_src.name\n",
    "  process_cnt -= 1\n",
    "  if process_cnt == 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sources/2206'] does exist!\n",
      "return_code = 256, ['sources/2206/2206.09112v4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: sources/2206/2206.09112v4: unexpected end of file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import system as system_command\n",
    "\n",
    "def checkAndCreateFolder(dirpath: str,\n",
    "                         filename: str=None, \n",
    "                         basepath: str=\"sources\",\n",
    "                         demo: bool=False\n",
    "                        ) -> bool:\n",
    "  '''\n",
    "  return `True` if the file exists.\n",
    "  '''\n",
    "  base_path = Path(basepath)\n",
    "  folder_path = Path(basepath).joinpath(dirpath)\n",
    "  file_path = folder_path.joinpath(filename)\n",
    "  if not base_path.exists():\n",
    "    base_path.mkdir()\n",
    "  if not folder_path.exists():\n",
    "    # print(f\"['{folder_path}'] doesn't exist. Creating one for you...\")\n",
    "    print(f\"['{folder_path}'] doesn't exist. \", end=\"\" if not demo else \"\\n\")\n",
    "    if not demo:\n",
    "      print(\"Creating one for you...\")\n",
    "      folder_path.mkdir()\n",
    "  else:\n",
    "    if demo:\n",
    "      print(f\"['{folder_path}'] does exist!\")\n",
    "  \n",
    "  # print(\"return code = \", system_command(f\"gunzip -t '{file_path}' > /dev/null\"))\n",
    "  return_code = system_command(f\"gunzip -t '{file_path}' > /dev/null\")\n",
    "  if return_code:\n",
    "    print(f\"{return_code = }, ['{file_path}']\")\n",
    "  # gunzip -t 'sources/2206/2206.09112v4'\n",
    "  # gunzip -c 'sources/2206/2206.09112v4' | tar t > /dev/null\n",
    "  # tar -tzf 'sources/2206/2206.09112v4' > /dev/null\n",
    "\n",
    "  return file_path.exists()\n",
    "\n",
    "# _paper_src_name = \"2005.05005v2\"\n",
    "_paper_src_name = \"2206.09112v4\"\n",
    "checkAndCreateFolder(dirpath=_paper_src_name.split(\".\")[0], filename=_paper_src_name, demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_src(arxiv_id: list, \n",
    "                 basepath: str=\"sources\"\n",
    "                 ) -> dict:\n",
    "  '''\n",
    "  Check if the directory exists, using arXiv API to download the `src`(latex included).\n",
    "  \n",
    "  ## returns\n",
    "  paper_info: dict\n",
    "  '''\n",
    "  search_by_id = arxiv.Search(id_list=arxiv_id)\n",
    "  client = arxiv.Client(page_size=500, delay_seconds=3, num_retries=3)\n",
    "  pbar = tqdm(client.results(search_by_id), \n",
    "              # total=probe_interval.stop - probe_interval.start,\n",
    "              total=len(arxiv_id),\n",
    "              miniters=1,\n",
    "              mininterval=0,\n",
    "              desc=\"Starting process...\"\n",
    "              )\n",
    "  skipped_list = []\n",
    "  paper_info: dict = {}\n",
    "\n",
    "  for paper in pbar: \n",
    "    paper_id = paper.get_short_id()\n",
    "    pbar.set_description(f\"Processing {paper_id}\")\n",
    "    paper_info[paper_id] = {}\n",
    "    paper_info[paper_id][\"name\"] = paper_id\n",
    "    paper_info[paper_id][\"title\"] = paper.title\n",
    "    if not checkAndCreateFolder(dirpath=paper_id.split(\".\")[0], filename=paper_id, basepath=PATH_TO_DOWNLOAD):\n",
    "      # Skip the task if the file already exist.\n",
    "      pbar.write(f\"{pbar.n+1}: [{paper_id}] {paper.title}\")\n",
    "      paper.download_source(dirpath=Path(basepath).joinpath(paper_id.split(\".\")[0]), filename=paper_id)\n",
    "    else:\n",
    "      skipped_list.append(paper_id)\n",
    "\n",
    "  # print(f\"{len(arxiv_id)} documents to process ({len(skipped_list)} were skipped).\")\n",
    "  pbar.write(f\"{len(arxiv_id)} documents to process ({len(skipped_list)} were skipped).\")\n",
    "\n",
    "  return paper_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Crawler Interval: [0, 200] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662f3fe752d34b46ac45820f5d220f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [200, 400] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b89bacc9514fe195eb18b11395eff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [400, 600] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32281da37434ff396649c067eda4052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [600, 800] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba94f79705b9425c8574ec94b4b5143c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [800, 1000] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418074dd9924d66902e406ef5f3f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [1000, 1200] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474a4e18929a4f62be30a94bb184bc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [1200, 1400] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28017ef1634f44eaa6f0668a64b3c62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [1400, 1600] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bdfed110604376a96371609f6fff8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [1600, 1800] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a094ca7aa4761a94b520bd7a1fbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [1800, 2000] -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2d0e755a324a2db5533ca8c5d64b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_code = 256, ['sources/2206/2206.09112v4']\n",
      "200 documents to process (200 were skipped).\n",
      "----- Crawler Interval: [2000, 2100] -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: sources/2206/2206.09112v4: unexpected end of file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aa53d8954447c2bce1633ef99ef420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting process...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 documents to process (100 were skipped).\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "RECORD_FMT: str = \"csv\" # \"json\"\n",
    "ITER_STEP: int = 200\n",
    "start_flag: int = 0\n",
    "\n",
    "while start_flag <= process_cnt_bak:\n",
    "  if start_flag + ITER_STEP >= process_cnt_bak:\n",
    "    probe_interval = slice(start_flag, process_cnt_bak)\n",
    "  else:\n",
    "    probe_interval = slice(start_flag, start_flag + ITER_STEP)\n",
    "  paper_id_list = list(all_paper_dict.keys())[probe_interval]\n",
    "  print(f\"----- Crawler Interval: [{probe_interval.start}, {probe_interval.stop}] -----\")\n",
    "  part_paper_dict = download_src(arxiv_id=paper_id_list, basepath=PATH_TO_DOWNLOAD)\n",
    "  start_flag += ITER_STEP\n",
    "\n",
    "  # info_table = pd.DataFrame(part_paper_dict).T\n",
    "  # info_table.to_csv(f\"record_{probe_interval.start}-{probe_interval.stop}.{RECORD_FMT}\")\n",
    "\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
